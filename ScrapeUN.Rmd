---
title: "R Notebook"
output: html_notebook
---


```{r}
rm(list = ls())
source("./ScrapeUNfuncs.R")
```

Download the pdfs

```{r}
search_link = "http://tbinternet.ohchr.org/_layouts/15/TreatyBodyExternal/TBSearch.aspx?Lang=en&TreatyID=9&DocTypeID=5"
download_pdfs(search_link)
```

Select the ones that contain certain words and summarize them
```{r}

output_folder = "homelessness"
words = "homelessness, homeless"
select_pdfs(words, output_folder)
summarize_pdfs(output_folder, sentences=5)

output_folder = "housing"
words = "housing, poverty"
select_pdfs(words, output_folder)
summarize_pdfs(output_folder, sentences=5)
```
```{r}
output_folder = "homelessness"
summarize_pdfs(output_folder, sentences=5)
```



```{r}






textdata <- base::readRDS(url("https://slcladal.github.io/data/sotu_paragraphs.rda", "rb"))
# load stopwords
english_stopwords <- readLines("https://slcladal.github.io/resources/stopwords_en.txt", encoding = "UTF-8")
# create corpus object
corpus <- Corpus(DataframeSource(textdata))
# Preprocessing chain
processedCorpus <- tm_map(corpus, content_transformer(tolower))
processedCorpus <- tm_map(processedCorpus, removeWords, english_stopwords)
processedCorpus <- tm_map(processedCorpus, removePunctuation, preserve_intra_word_dashes = TRUE)
processedCorpus <- tm_map(processedCorpus, removeNumbers)
processedCorpus <- tm_map(processedCorpus, stemDocument, language = "en")
processedCorpus <- tm_map(processedCorpus, stripWhitespace)
```

